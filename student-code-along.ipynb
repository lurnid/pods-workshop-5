{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5: Clustering and Dimensionality Reduction\n",
    "## Student Practice Notebook\n",
    "\n",
    "This notebook accompanies the tutor's live demonstration. Run the cells as the tutor presents, and try the **\"Your turn\"** variations to reinforce your understanding.\n",
    "\n",
    "**Topics covered:**\n",
    "1. K-Means Clustering\n",
    "2. Evaluating Clusters (Elbow Method & Silhouette Score)\n",
    "3. PCA (Principal Component Analysis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create toy data\n",
    "\n",
    "We'll create a simple dataset with 3 natural clusters to demonstrate how K-Means works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data: a small, simple dataset used to illustrate a concept\n",
    "# Here: we create random points that naturally form 3 clusters\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the original cluster centres. We'll compare these to what K-Means finds\n",
    "original_centres = np.array([\n",
    "    [2, 2],  # Cluster A\n",
    "    [6, 6],  # Cluster B\n",
    "    [2, 6]   # Cluster C\n",
    "])\n",
    "\n",
    "# Create 3 clusters of points, each with a different centre\n",
    "cluster1 = np.random.randn(30, 2) * 0.6 + original_centres[0]\n",
    "cluster2 = np.random.randn(30, 2) * 0.6 + original_centres[1]\n",
    "cluster3 = np.random.randn(30, 2) * 0.6 + original_centres[2]\n",
    "\n",
    "# Combine all points into one dataset\n",
    "X = np.vstack([cluster1, cluster2, cluster3])\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"This means we have {X.shape[0]} data points, each with {X.shape[1]} features\")\n",
    "print(f\"\\nOriginal centres used to generate the data:\")\n",
    "for i, centre in enumerate(original_centres):\n",
    "    print(f\"  Cluster {chr(65+i)}: ({centre[0]:.2f}, {centre[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Visualise the unlabelled data\n",
    "\n",
    "This is what K-Means \"sees\": points with no labels. Can you spot the clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the UNLABELLED data. this is what clustering algorithms see\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='steelblue', s=50, alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Unlabelled Data. Can You See the Clusters?')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Clustering is UNSUPERVISED. We don't tell the algorithm where the groups are.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Apply K-Means (k=3)\n",
    "\n",
    "Now we'll use K-Means to find the clusters automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means in action: let's cluster our toy data\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a K-Means model with 3 clusters\n",
    "# n_clusters is a HYPERPARAMETER: we set it, the algorithm doesn't learn it\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# fit_predict: learns cluster centres and assigns each point to a cluster\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# The cluster centres (centroids) that K-Means found\n",
    "centres = kmeans.cluster_centers_\n",
    "\n",
    "print(f\"Cluster labels assigned: {np.unique(labels)}\")\n",
    "print(f\"\\nK-Means found these centres:\")\n",
    "for i, centre in enumerate(centres):\n",
    "    print(f\"  Cluster {i}: ({centre[0]:.2f}, {centre[1]:.2f})\")\n",
    "\n",
    "# Compare to the original centres we used to generate the data\n",
    "print(f\"\\nOriginal centres (for comparison):\")\n",
    "for i, centre in enumerate(original_centres):\n",
    "    print(f\"  Cluster {chr(65+i)}: ({centre[0]:.2f}, {centre[1]:.2f})\")\n",
    "\n",
    "print(\"\\n→ The algorithm recovered centres very close to the originals!\")\n",
    "print(\"  Slight differences are due to random variation in the data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the clustering result\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot points coloured by their assigned cluster\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "\n",
    "# Plot the cluster centres as red X markers\n",
    "plt.scatter(centres[:, 0], centres[:, 1], c='red', marker='X', s=200, \n",
    "            edgecolors='black', linewidths=2, label='Centroids')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-Means Clustering Result (k=3)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Each point is assigned to its NEAREST centroid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: Try different values of k\n",
    "\n",
    "What happens if we ask K-Means to find a different number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Change n_clusters to 2, then to 4, and observe what happens\n",
    "# How does the clustering change? Does k=2 or k=4 make sense for this data?\n",
    "\n",
    "kmeans_try = KMeans(n_clusters=2, random_state=42)  # <-- Try changing this to 2 or 4\n",
    "labels_try = kmeans_try.fit_predict(X)\n",
    "centres_try = kmeans_try.cluster_centers_\n",
    "\n",
    "# Visualise\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels_try, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.scatter(centres_try[:, 0], centres_try[:, 1], c='red', marker='X', s=200, \n",
    "            edgecolors='black', linewidths=2, label='Centroids')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(f'K-Means Clustering Result (k={len(centres_try)})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Evaluating Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Elbow Method (WCSS)\n",
    "\n",
    "How do we choose the right number of clusters? The **elbow method** plots WCSS (Within-Cluster Sum of Squares) for different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Elbow Method: finding the optimal number of clusters\n",
    "# WCSS = how spread out points are within their clusters (lower = tighter clusters)\n",
    "\n",
    "k_values = range(1, 11)  # Try k from 1 to 10\n",
    "wcss_values = []  # Store WCSS for each k\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    # inertia_ is sklearn's name for WCSS\n",
    "    wcss_values.append(kmeans.inertia_)\n",
    "\n",
    "print(\"WCSS for each k:\")\n",
    "for k, wcss in zip(k_values, wcss_values):\n",
    "    print(f\"  k={k}: WCSS = {wcss:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, wcss_values, 'bo-', linewidth=2, markersize=8)\n",
    "\n",
    "# Highlight the elbow point (k=3 in our case)\n",
    "plt.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Elbow point (k=3)')\n",
    "\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "plt.title('Elbow Method: Finding Optimal k', fontsize=14)\n",
    "plt.xticks(k_values)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The 'elbow' is where adding more clusters stops giving big improvements.\")\n",
    "print(\"Here, the elbow is at k=3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Silhouette Score\n",
    "\n",
    "Another way to evaluate clustering quality. Measures how similar points are to their own cluster vs other clusters.\n",
    "\n",
    "- **+1** = perfect clustering\n",
    "- **0** = overlapping clusters  \n",
    "- **-1** = points in wrong cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Silhouette Score — another way to evaluate clustering quality\n# Measures how similar points are to their own cluster vs other clusters\n# Range: -1 (bad) to +1 (good), 0 means overlapping clusters\n\n# How it works (for each point):\n#   a = avg distance to points in SAME cluster (cohesion)\n#   b = avg distance to points in NEAREST OTHER cluster (separation)\n#   silhouette = (b - a) / max(a, b)\n# If b >> a → score near +1 (good), if a >> b → score near -1 (bad)\n\nfrom sklearn.metrics import silhouette_score\n\n# Calculate silhouette score for different values of k\n# Note: silhouette score requires at least 2 clusters\nk_values_sil = range(2, 11)\nsilhouette_values = []\n\nfor k in k_values_sil:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)\n    score = silhouette_score(X, labels)\n    silhouette_values.append(score)\n\nprint(\"Silhouette scores for each k:\")\nfor k, score in zip(k_values_sil, silhouette_values):\n    print(f\"  k={k}: Silhouette = {score:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(list(k_values_sil), silhouette_values, 'go-', linewidth=2, markersize=8)\n",
    "\n",
    "# Highlight the best k (highest silhouette score)\n",
    "best_k = list(k_values_sil)[np.argmax(silhouette_values)]\n",
    "best_score = max(silhouette_values)\n",
    "plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Best k={best_k} (score={best_score:.3f})')\n",
    "\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Silhouette Score', fontsize=12)\n",
    "plt.title('Silhouette Method: Finding Optimal k', fontsize=14)\n",
    "plt.xticks(list(k_values_sil))\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHighest silhouette score is {best_score:.3f} at k={best_k}\")\n",
    "print(\"Higher is better: both methods agree that k=3 is optimal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: Interpret the results\n",
    "\n",
    "Look at the elbow plot and silhouette scores above:\n",
    "- Do both methods agree on the optimal k?\n",
    "- What would happen if they disagreed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create correlated data\n",
    "\n",
    "PCA is useful when features are correlated (contain redundant information). Let's create some data to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Demo: reducing dimensions while keeping the important information\n",
    "# First, let's create some correlated data with 4 features\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a base variable\n",
    "base = np.random.randn(100)\n",
    "\n",
    "# Create 4 features: some are correlated with each other\n",
    "feature1 = base + np.random.randn(100) * 0.2          # Strongly related to base\n",
    "feature2 = base * 0.8 + np.random.randn(100) * 0.3    # Also related to base\n",
    "feature3 = np.random.randn(100)                        # Independent\n",
    "feature4 = feature3 * 0.5 + np.random.randn(100) * 0.4  # Related to feature3\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df_pca = pd.DataFrame({\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Feature4': feature4\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df_pca.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Check correlations\n",
    "\n",
    "A correlation heatmap shows which features are related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations between features\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_pca.corr(), annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Features 1 and 2 are very highly correlated (~0.97),\")\n",
    "print(\"and Features 3 and 4 are also strongly correlated (~0.79).\")\n",
    "print(\"PCA can help reduce this redundancy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Apply PCA\n",
    "\n",
    "We'll reduce from 4 features to 2 principal components.\n",
    "\n",
    "**Important:** Always scale your data before PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce from 4 dimensions to 2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Standardise the data (important for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pca)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original shape: {df_pca.shape}\")\n",
    "print(f\"After PCA:      {X_pca.shape}\")\n",
    "print(f\"\\nWe reduced from 4 features to 2 principal components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 How much information did we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_variance_ratio_ tells us how much variance each component captures\n",
    "\n",
    "print(\"Variance explained by each principal component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.1%}\")\n",
    "\n",
    "total_var = sum(pca.explained_variance_ratio_)\n",
    "print(f\"\\nTotal variance explained by 2 components: {total_var:.1%}\")\n",
    "print(f\"\\nWe kept {total_var:.1%} of the information while halving the number of features!\")\n",
    "print(\"\\nThis makes sense: we had two pairs of correlated features (1-2 and 3-4),\")\n",
    "print(\"so two components can capture the essential structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Visualise in PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the data in the new 2D PCA space\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c='steelblue', s=50, alpha=0.7)\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('Data Projected onto First 2 Principal Components', fontsize=14)\n",
    "plt.axhline(y=0, color='grey', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='grey', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"PC1 captures the main direction of variation in the data.\")\n",
    "print(\"PC2 captures the second most important direction (perpendicular to PC1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn: Try different numbers of components\n",
    "\n",
    "What if we only keep 1 component? Or keep 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Change n_components to 1 or 3 and see how variance explained changes\n",
    "\n",
    "pca_try = PCA(n_components=1)  # <-- Try changing this to 1 or 3\n",
    "X_pca_try = pca_try.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Components: {pca_try.n_components_}\")\n",
    "print(f\"Shape after PCA: {X_pca_try.shape}\")\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "for i, var in enumerate(pca_try.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.1%}\")\n",
    "print(f\"\\nTotal variance explained: {sum(pca_try.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Quick Reference\n",
    "\n",
    "## K-Means Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "kmeans.cluster_centers_  # Centroid coordinates\n",
    "kmeans.inertia_          # WCSS\n",
    "```\n",
    "\n",
    "## Evaluation Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "score = silhouette_score(X, labels)  # -1 to +1, higher is better\n",
    "```\n",
    "\n",
    "## PCA Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Always scale first!\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca.explained_variance_ratio_  # Variance per component\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}